---
title: "RLHF Basics"
date: 2025-02-13
categories:
  - blog
tags:
  - Alignment
  - LLM
---

The goal of Reinforcement Learning from Human Feedback (RLHF) is aligning LLMs with human expectation. This post explains some of its basics.

